#  -*- mode: shell-script -*-

# TODO: fix utail to allow args:
# > ueach -a 'production-collab-[1-9]' -- "sudo sh -c \"tail -Fn0 /var/log/nginx/genesis-proxy.access.log\"" | pv -lra >/dev/null

[[ "$AWS_ENV" == development* ]] && awsregion us-west-1

EC2_ROLE="${EC2_ROLE#fullstack-}"

if [ -n "$EC2_ROLE" ]; then
    SECRET_FILE="$(find /var/opt -name "${EC2_ROLE//-jobs}-secret_token.${EC2_ENV}" | xargs -r ls -t | head -1)"
    [ ! -f "$SECRET_FILE" ] && \
        SECRET_FILE="$(ls -1t "/var/opt/unity-${EC2_ROLE}/"*."${EC2_ENV}" 2>/dev/null | head -1)"
    [ -f "$SECRET_FILE" ] && . "$SECRET_FILE" || unset SECRET_FILE
fi

# export HEKETI_CLI_SERVER='http://gluster-api.us-west-1.unityops.net:8080'
export SPEED=1

if [[ -f "${HOME}/.ssh/id_github" ]]; then
    export GH_SSH_KEY_PATH="${HOME}/.ssh/id_github"
    export GH_SSH_KEY=$GH_SSH_KEY_PATH
fi

# ensure collab's bundler is used
RVM_PIN_RUBY244GLOBAL+=(bundler:$(sed -n '/^BUNDLED WITH/{n;p;}' \
                                      ~/work/unitycloud-collab-service/Gemfile.lock | \
                                      tr -d '[:space:]'))

# must be in "root" not in @global for emacs to work
RVM_PIN_DEFAULT+=(rubocop:$(awk '/^ *rubocop /{gsub(/[()]/,"",$2); print $2; exit}' \
                                ~/work/unitycloud-collab-service/Gemfile.lock))

alias reset_dev='rm -rfv devrepos/* db/*.sqlite3; mysql -uroot -e "drop database unity_cloud_collab"; rake db:create db:migrate'
alias mcls='ruby -I /opt/unity/unity-collab-service/current/vendor/bundle/ruby/2.3.0/gems/dalli-2.7.6/lib ~/bin/memcache_list_keys'
alias list_editor_args='find Editor/Src -type f -name "*.cpp" -print0 | xargs -0 grep -F HasARGV'
alias gitsum='git ls-files -z | xargs -0 grep -F size | pipesum 10000 2'

alias kodd='k repl ^odd'
alias krcs='k -n sst-collab repl ^odd rails c -s'
alias krc='k -n sst-collab repl ^odd rails c'

if $IAMME && $INTERACTIVE && ihave _z; then
    zf="${_Z_DATA:-${HOME}/.z}"
    touch "$zf"
    shopt -s nullglob # empty list if no match
    for d in /opt/unity/unity*; do
        n="$(basename "$d" | cut -d- -f2-)"
        [ -d "${d}/current" ] && d="${d}/current"
        # only add once to avoid increasing the frecency value
        grep -q "$d" "$zf" || _z --add "$d" 2>/dev/null
    done
    shopt -u nullglob
fi

function opsme()
{
    local gitroot=$(git root)
    [[ -n "${gitroot}" ]] || return 1
    local bindir="$(git root)/bin"
    if [[ ! -d "${bindir}" ]]; then
        echo "run from within an ops repo: tried ${bindir}" >&2
        return 2
    fi
    if [[ "${UNITYCLOUDOPS}" = "${bindir}" ]]; then
        echo "already using ${bindir}" >&2
        return 3
    fi
    if [[ -n "${UNITYCLOUDOPS}" ]]; then
        echo "replacing ${UNITYCLOUDOPS} with ${bindir}" >&2
        PATH=$(echo "${PATH}" | sed "s|:${UNITYCLOUDOPS}||")
    else
        echo "setting ${bindir}" >&2
    fi
    export UNITYCLOUDOPS=${bindir}
    exec bash -ic "PATH=\"${PATH}:${bindir}\" bash"
}

function rspf()
{
    SPEED=1 rake "parallel:spec[$*,,--fail-fast --format progress --format ParallelTests::RSpec::FailuresLogger --out tmp/failing_specs.log]"
}

function rspp()
{
    SPEED=1 rake "parallel:spec[$*,,--format progress --format ParallelTests::RSpec::FailuresLogger --out tmp/failing_specs.log]"
}

if $DARWIN; then
    function unity_bin()
    {
        local dn bn
        bn='Contents/MacOS/Unity'
        if [ -x "$bn" ]; then
            echo "bn"
            return 0
        fi
        for dn in '.' build/MacEditor /Applications/Unity*; do
            dn+="/Unity.app"
            if [ -d "$dn" ]; then
                echo "${dn}/${bn}"
                return 0
            fi
        done
        echo "Unable to locate Unity binary" >&2
        return 1
    }

    # if last arg is a path, will open it as a project
    function unity_direct()
    {
        local pdir args=("$@")
        if [[ ${#args[@]} -gt 0 && -d "${args[${#args[@]}-1]}" ]]; then
            pdir="${args[${#args[@]}-1]}"
            unset args[${#args[@]}-1]
            # unity requires full paths
            pdir="$(cd "$pdir" && pwd)"
            # this is how to open more than one unity project
            args+=(-projectPath "$pdir")
        fi
        local ub
        ub="$(unity_bin)"
        echo "${ub} $(shellwords "${args[@]}") >&2"
        tailrun -p "predate $ISO8601_FMT" "${HOME}/Library/Logs/Unity/Editor.log" "${ub}" "${args[@]}"
    }

    function unity_hub()
    {
        if [[ $# -lt 1 ]]; then
            echo 'usage: unity_hub { prod | stag | dev | local }'
            return 1
        fi
        local cloud_env=$1; shift
        case "$cloud_env" in
            prod*) cloud_env=production;;
            stag*) cloud_env=staging;;
            dev*|andy*) cloud_env=dev;;
            local) cloud_env=local;;
            *)
                echo "unknown cloud env name: ${cloud_env}" >&2
                return 2
        esac
        tailrun -p "predate $ISO8601_FMT" "${HOME}/Library/Logs/Unity/Editor.log" \
                /Applications/Unity\ Hub.app/Contents/MacOS/Unity\ Hub -- --cloudEnvironment "${cloud_env}"
    }
fi

function health()
{
    local pn="/opt/unity/unity-${EC2_ROLE}-service/current"
    local px
    [ ! -d "$pn" ] && pn="/opt/unity/unity-${EC2_ROLE}/current"
    if [ ! -d "$pn" ]; then
        pn='/opt/unity/unity-collab-cache/current'
        px='localhost/csapi/health'
    else
        px='localhost:8080/api/health'
    fi
    myps 'unicorn|collabcache'
    ls -ld `readlink "$pn"`
    local h="$(curl -sqf -u "build:${BUILD_SERVICE_SECRET}" ${px})"
    echo "$h"
    eval `echo "$h" | sed -n 's/.*version":"\([^"]*\).*/local version=\1/p'`
    echo -n "${version}  ->"
    (cd "$pn"; git branch -r --contains "$version"; git log -n1)
}

function whitelist_check()
{
    # TODO: _any_ access token will work, so snag the first one in memcache not expired...
    if [ -z "$ACCESS_TOKEN" -o $# -lt 1 ]; then
        echo 'usage: ACCESS_TOKEN=sekritz whitelist_check <org_fk> [<org_fk>...]' >&2
        return 1
    fi
    local org_fks="$(join , "$@")"
    curl -H "Authorization: Bearer $t" "localhost:5001/v1/alpha-features/api/whitelist/COLLAB/${org_fks}"
    echo
}

function core()
{
    local cargs
    local user_fk='collab'
    if [ "$1" = '-v' ]; then
        shift; cargs='-v'
    else
        cargs='-qsf'
    fi
    if [ "$1" = '-u' ]; then
        shift; user_fk=$1; shift
    fi
    if [ $# -ne 1 ]; then
        echo 'usage core [-v] [-u <user_fk>] <path>'
        return 1
    fi
    curl $cargs -L -u "${user_fk}:${CORE_API_SECRET}" "https://${AWS_ENV_PREFIX}core.cloud.unity3d.com$1"
    local rc=$?
    echo
    return $?
}

function sshrails()
{
    local rhost srvc
    if [ $# -lt 1 ]; then
        echo 'usage: sshrails <remote_host> [<rails_options>...]' >&2
        return 1
    fi

    rhost="$1"; shift

    \ssh -t "$rhost" bash -ic "'
sudo -u nobody -i bash -c \"\
cd /opt/unity/unity-\${EC2_ROLE}-service/current;\
. \$SECRET_FILE;\
export RAILS_ENV=\$EC2_ENV;\
bin/rails "$@"\"'"
}

function volfor()
{
    local pfk="$(downcase "$1")"
    local shard="${pfk:0:2}"
    if $IAMME; then
        sudo su -c 'ls -1dt /mnt/repos/volume*'/${shard}/${pfk} | head -1
    else
        ls -1dt /mnt/repos/volume*/${shard}/${pfk} | head -1
    fi
}

function beanview()
{
    local walk=false
    if [[ "$1" = '--walk' ]]; then
        shift; walk=true
    fi

    if [[ $# -ne 1 ]]; then
        echo 'usage: beanview [--walk] <beanstalk_pod_index>' >&2
        return 1
    fi

    k -n sst-common port-forward pod/beanstalkd-$1 11300:11300 &
    sleep 3
    if $walk; then beanwalker; else beanstool stats; fi
    local rc=$?
    pkill -f -- "sst-common port-forward pod/beanstalkd-$1"
    return $rc
}

function invalidate_cdn()
{
    if [ $# -lt 1 ]; then
        echo 'usage: invalidate_cdn <env> [<env> ...]' >&2
        return 1
    fi

    local objects=()
    while [ $# -gt 0 ]; do
        objects+=('"https://public-cdn.cloud.unity3d.com/config/'$1'"')
        shift
    done

    http -va "public-cloud-api@unity3d.com:${CDN_API_SECRET}" \
         https://api.ccu.akamai.com/ccu/v2/queues/default \
         'action:="invalidate"' 'objects:=['"$(join , "${objects[@]}")"']'
}

if ihave papertrail; then
    alias pt-prod='pt -g "Collab Production"'
fi

if [ "$(id -un)" = 'nobody' ]; then
    function collab_exec()
    {
        z current && ./bin/bundle exec "$@"
    }

    function clone_project()
    {
        git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1
    }
else
    function collab_exec()
    {
        sudo -u nobody bash -ic "cd '$(z -e current)' && ./bin/bundle exec $(shellwords "$@")"
    }

    function clone_project()
    {
        sudo -u nobody git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1 && sudo chown -R "$(id -un):" .
    }
fi

function collab_rbtrace_workers
{
    collab_exec rbtrace --ps='unicorn worker' -e "$@"
}

function collab_rbtrace_master
{
    collab_exec rbtrace --ps="$(pgrep -f unicorn\ master)" -e "$@"
}


# each_worker?
# > kubey -c usw1 -n staging collab/collab each -ap -- 'for p in `pgrep -f unicorn\ worker`; do echo $p; kill -QUIT $p; done; wait'

# repl_first...
# > kubey -c usw1 -n staging -m1 coll/coll repl bash


# TODO: add helper for running this:
# ueach -a -p production-collab-\\d+ -- 'sudo -u nobody bash -ic "/opt/unity/unity-collab-service/current/bin/bundle exec rbtrace -p \$(pgrep -f unicorn\ master) -e Rails.configuration.memcached_client"'

function collab_loglevel()
{
    collab_rbtrace "Rails.logger.level = :$1"
}

# TODO:
# upid="$(basename `pwd -P`)"
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1}}' > ~/${upid}.md5s)
# one-time:
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1};fflush();}' | s3-unarchive $(basename $PWD))

# TODO: add easy ability to convert only one file!
function convert_pointers()
{
    local download=false
    if [ "$1" = '--download' ]; then
        download=true; shift
    fi

    local max_failures=10
    if [ "$1" = '--max-failures' ]; then
        shift; max_failures=$1; shift
    fi

    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers [--download] [--max-failures <count>] <directory>' >&2
        return 1
    fi

    # convert dev-user to be dev/user for bucket
    local bucket="unitycloud-collab-store-${AWS_ENV/-${USER}//${USER}}"
    local dir="$(cd "$1" && pwd -P)"; shift
    local pfk="$(basename "$dir")"
    local failures=0

    local total=`git ls-files "$dir" | wc -l`
    local count=0
    # read using NUL terminators to properly handle UTF-8 encoding of file names
    while read -r -d $'\0' fn; do
        (( count++ ))
        md5="$(awk '/^uc_md5/{print $2}' "$fn")"
        if [ -z "$md5" ]; then
            echo "No MD5 found in ${fn}" >&2
            (( failures++ ))
        elif $download; then
            mv -v "$fn" "${fn}.pointer"
            key="${bucket}/${pfk}/${md5}"
            if ! aws s3 cp "s3://$key" "$fn"; then
                mv -v "${fn}.pointer" "$fn"
                (( failures++ ))
            fi
        else
            key="${pfk}/${md5}"
            if ! aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                # echo "Missing ${bucket}/${key} in ${fn}" >&2
                key="collab_archive_${pfk}/${md5}"
                if aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                    echo "${fn} is ARCHIVED as ${bucket}/${key}" >&2
                else
                    echo "${fn} is MISSING ${bucket}/${key} (and is not archived)" >&2
                    (( failures++ ))
                fi
            fi
        fi
        if [ $failures -ge $max_failures ]; then
            echo "Giving up after ${failures} failures" >&2
            break
        fi
        [ $(( $count % 10 )) -eq 0 ] && echo "Processed ${count} / ${total}"
    done < <(git ls-files -z)

    [ $failures -eq 0 ] && return 0

    echo "Summary: ${failures} failures reported"
    return 2
}

function unconvert_pointers()
{
    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers <directory>' >&2
        return 1
    fi

    find "$1" -name .git -prune -o -type f -name '*.pointer' -print | while read fn; do
        dir="$(dirname "$fn")"
        orig="${dir}/$(basename "$fn" .pointer)"
        mv -vf "$fn" "$orig"
    done
}

# moves all files found in transaction folders out into the project root
function convert_transactions()
{
    if [ $# -lt 1 ]; then
        echo 'usage: convert_transactions <project_id> [<project_id>...]' >&2
        return 1
    fi

    echo 'TODO: make converter for old transaction folders in s3!'
}

# reports the following space-delimited fields for ONLY project/md5 requests:
#   <timestamp> <remote_ip> <http_method> <project_id> <file_md5> <http_status> <bytes_sent>
# see-also: http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html
function simplified_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: simplified_s3_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | pipejoin $'\3' | awk -F$'\3' '$9 ~ /\// {
sub(/\[/,"",$3); sub(/\]/,"",$4); gsub("/"," ",$9); split($8,a,".");
print $3 $4, $5, a[2], $9, $10, $11, $13, $(NF-1);
}'
}

function split_s3_key_from_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: split_s3_key_from_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | awk '$9 ~ /\//{gsub("/"," ",$9);a[$9]++} END{for (n in a) print n}'
}

function get_project_ids_from_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: get_project_ids_from_s3_logs <log> [<log>...]' >&2
        return 1
    fi
    split_s3_key_from_log "$@" | awk '{print $1}' | sort | uniq -c | sort -n
}

function report_project_versions()
{
    local lfn
    [ -f "$1" ] && lfn="$1" || lfn='-'

    if test $# -eq 0 && istty stdin; then
        echo 'usage: report_project_versions [<access_log_file>]' >&2
        return 1
    fi

    while read -r line; do
        res="$(echo "$line" | sed -n 's/^.*projects\/\([^\/]*\).*UnityEditor\/\([^ ]*\).*/upid="\1";ver="\2"/p')"
        eval "$res"
        if [ -z "$upid" -o -z "$ver" ]; then
            continue
        fi
        tag="$(hg_user_agent_revision -tag "$ver")"
        rc=$?
        if [ -z "$tag" -o $rc -ne 0 ]; then
            echo "Failed to get tag for ${ver}" >&2
        else
            echo "${upid}:${tag}:${ver}"
        fi
    done < <(cat "$lfn")
}

function statsd_report()
{
    local host='internal-stats.cloud.unity3d.com'
    if [ "$1" = '-h' ]; then
        shift; host="$1"; shift
    fi
    # https://github.com/b/statsd_spec | https://github.com/etsy/statsd/blob/master/docs/metric_types.md
    declare -A cmds
    cmds=([count]=c [gauge]=g [time]=ms [hist]=h [meter]=m [set]=s)
    local cmd=${cmds[$1]}
    local keys
    if [ $# -lt 2 -o -z "$cmd" ]; then
        keys="$(join '|' `echo "${!cmds[@]}"`)"
        echo "usage: stats_report [-h <host>] {${keys}} <metric> [<value>] [<sample_rate>]" >&2
        return 1
    fi
    local msg="${HOSTNAME//./_}.${2}:${3:-1}|${cmd}"
    [ -n "$4" ] && msg+="|@$4"
    echo "$msg"
    echo "$msg" | nc -uw0 "$host" 8125
}

function beanstool()
{
    local beanbin="${HOME}/bin/beanstool"
    if [ -f /etc/default/beanstalkd ]; then
        .  /etc/default/beanstalkd
    else
        BEANSTALKD_LISTEN_ADDR="$(get_iface_ip inet eth0)"
        BEANSTALKD_LISTEN_PORT='11300'
    fi
    if [ ! -x "$beanbin" ]; then
        (
            set -e
            cd "${HOME}/bin"
            ver='0.2.0'
            fn="beanstool_v${ver}_$(downcase "$UNAME")_amd64"
            wget "https://github.com/src-d/beanstool/releases/download/v${ver}/${fn}.tar.gz"
            tar zxf "${fn}.tar.gz" --strip-components=1 "${fn}/beanstool"
            rm -f "${fn}.tar.gz"
        )
    fi
    "$beanbin" "$@" --host="${BEANSTALKD_LISTEN_ADDR}:${BEANSTALKD_LISTEN_PORT}"
}

function instances_needed()
{
    if [ $# -lt 3 ]; then
        echo 'usage: instances_needed <req_per_second> <ms_latency> <num_workers>' >&2
        return 1
    fi
    calc "$1 / ((1000 / $2) * $3)"
}

function workers_needed()
{
    if [ $# -lt 3 ]; then
        echo 'usage: workers_needed <req_per_second> <ms_latency> <instances>' >&2
        return 1
    fi
    calc "($1 / (1000 / $2)) / $3"
}

function progress_report()
{
    if [ $# -ne 3 ]; then
        echo 'usage: progress_report <format> <count_interval> <total>' >&2
        return 1
    fi
    [ $(($3 % $2)) -eq 0 ] && printf "$1" $3
    return 0
}

function s3_remove_zero_byte_files()
{
    local bucket search total line_count last_upid line size key upid md5
    if [ $# -lt 1 ]; then
        echo 'usage: s3_remove_zero_byte_files <bucket> [<upid> ...]' >&2
        return 1
    fi

    bucket="$1"; shift
    if [ $# -gt 0 ]; then
        search=()
        for upid in "$@"; do search+=($upfid); done
    else
        search=("$bucket")
    fi

    total=0
    line_count=0
    for item in "${search[@]}"; do
        last_upid=''
        while read -r line; do
            line=($line)
            size="${line[2]}"
            [ "$size" -eq 0 ] || continue
            # ts="${line[0]} ${line[1]}"
            key="${line[3]}"
            upid="${key%%/*}"
            md5="${key##*/}"
            if [ "$last_upid" != "$upid" ]; then
                last_upid="$upid"
                echo "checking ${upid}"
            fi
            progress_report "removed ${total} of %d\n" 2000 $((++line_count))
            if [ -z "$md5" ] || [ "$md5" = 'd41d8cd98f00b204e9800998ecf8427e' ]; then
                # ignore directory only or legit zero byte file
                continue
            fi
            echo "${line[*]}"
            aws s3 rm "s3://${bucket}/${key}"
            (( total++ ))
        done < <( aws s3 ls --recursive "s3://$item" )
    done
    echo "Removed ${total} of ${line_count} items from ${bucket}"
}

function klean_evictions()
{
    if [[ $# -ne 1 ]]; then
        echo 'usage: klean_evictions <context>' >&2
        return 1
    fi
    local ctx=$1
    kubectl --context ${ctx} get pod --all-namespaces -a | \
        ruby -e 'b=Hash.new{|h,k|h[k]=[]};'`
                `'while a=gets&.split;'`
                  `'a[3] =~ /Evicted/ and b[a[0]] << a[1];'`
                `'end;'`
                `'b.each{|(k,v)|'`
                  `"puts %x{kubectl --context ${ctx} delete pod -n #{k} #{v.join(' ')}}"`
                `'}'
}

# TODO: add helper to get currently env deployment tag and open URL to compare to master:
# https://gitlab-prod1.eu-cph-1.unityops.net/cloudservices/collab-service/compare/20170314-Production...master

# TODO: support by node name (e.g. node-7, node-8, etc)
function kssh()
{
    local user=$USER
    local kubey_args

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -u) shift; user=$1;;
            -k) shift; kubey_args=$1;;
            *)  break;;
        esac
        shift
    done

    if [[ $# -lt 1 ]]; then
        echo 'usage: kssh [-u <user>] [-k <kubey_args>] <match> [<ssh_args> ...]' >&2
        return 1
    fi

    local match=$1; shift
    local node_ip
    node_ip=($(kubey $kubey_args -f plain --no-headers "$match" list -c host_ip | \
                   cut -d' ' -f1 | sort -u))
    if [[ ${#node_ip[@]} -ne 1 ]]; then
        echo "Invalid node IP found: ${node_ip[*]}" >&2
        return 2
    fi

    ssh "${user}@${node_ip}" "$@"
}

function clean_orphan_replicasets()
{
    local kargs=(-n "$1")
    [[ $# -gt 1 ]] && kargs+=(--context "$2")
    if [[ $# -lt 1 ]]; then
        echo 'usage: clean_orphan_replicasets <namespace> [<context>]' >&2
        return 1
    fi
    kubectl "${kargs[@]}" get replicasets | grep ' 0  *0  *0 ' | tee /dev/tty | awk '{print $1}' | \
        xargs kubectl "${kargs[@]}" delete replicaset
}

function collab_deploy()
{
    local defpods='oddjob backburner sqsprocessor collab collabot'
    if [[ $# -lt 2 ]]; then
        echo "usage: collab_deploy <env> <img> [${defpods}]" >&2
        return 1
    fi
    local env=$1; shift
    local img=$1; shift
    local pods="${*}"
    [[ -z "$pods" ]] && pods=$defpods
    local pod fullimg
    for pod in $pods; do
        echo "[$pod]"
        fullimg="artifactory.eu-cph-1.unityops.net:5010/collab:$img"
        ./deploy_collab.sh --stable --image "$fullimg" apply "$env" "$pod" || break
        if [[ "$pod" = 'backburner' ]]; then
            ./deploy_collab.sh --stable --image "$fullimg" apply "$env" backburner-simple || break
        fi
    done
}

function watch_deploy()
{
    if [[ $# -lt 1 ]]; then
        echo 'usage: watch_deploy <kubey_args>' >&2
        return 1
    fi
    local epid="$(mktemp)"
    local i
    ( kubey "$@" . events & echo $! >&3 ) 3>"$epid" | grep -v -E 'DNS|Resolv' &
    ( # run in a subshell to trap control-c keyboard interrupt
        trap "echo killing $(<$epid); kill $(<$epid); rm -f $epid" 0
        while kubey --cache-seconds 5 "$@" .; do
            echo '======================================================================'
            i=10
            while [[ $i -gt 0 ]]; do
                printf "Refresh in %d seconds...                                      \r" $i
                sleep 1
                (( i-- ))
            done
            printf "                                                                      \r"
        done
    )
}

function ktrace()
{
    if [[ $# -ne 2 ]]; then
        echo 'usage: ktrace <kubey_args> <trace_cmd>' >&2
        return 1
    fi
    local kargs="$1"; shift
    local tcmd="$1"; shift
    local cmd='for p in `pgrep -f unicorn\ worker`; do '
    cmd+='( bin/bundle exec rbtrace -p $p -e "'"$tcmd"'" 2>/dev/null | grep -F "=>" & ); '
    cmd+='done; wait'
    kubey $kargs each -ap -- $cmd
}

function check_consul_available()
{
    if [[ $# -lt 1 ]]; then
        echo 'usage check_consul_available <ctx> <env>' >&2
        return 1
    fi
    local of="/tmp/consul_available_${1}_${2}.log"
    ktrace "-c $1 -n collab-$2 collab/collab" '[Process.pid,RuntimeConfig.consul_available?]' | \
        tee "$of" | highlight false
    local false_cnt true_cnt
    true_cnt=$(grep -cF true "$of")
    false_cnt=$(grep -cF false "$of")
    echo "Saved output to $of (${true_cnt} connected and ${false_cnt} disconnected)"
}

# # https://www.percona.com/blog/2012/08/29/heres-a-quick-way-to-foresee-if-replication-slave-is-ever-going-to-catch-up-and-when/
# s_behind – current Seconds_Behind_Master value
# d_behind – number of days behind based on current s_behind
# c_sec_s – how many seconds per second were caught up during last interval
# eta_d – this is ETA based on last interval
# O_c_sec_s – overall catch-up speed in seconds per second
# O_eta_d – ETA based on overall catch-up speed (in days)
# O_eta_h – same like previous but in hours
function mysql-slave-status-monitor()
{
    if [[ $# -ne 1 ]]; then
        echo 'usage: mysql-slave-status-monitor <interval-seconds>' >&2
        return 1
    fi
    local delay=$1
    local cmd="$SUDO mysql -e 'show slave status\G' | grep Seconds_Behind_Master | awk '{print \$2}'"
    while sleep $delay; do
        eval $cmd
    done | awk -v delay=$delay '
{
   passed += delay;
   if (count%10==0)
      printf("s_behind d_behind   c_sec_s   eta_d | O_c_sec_s O_eta_d O_eta_h\n");
   if (prev==NULL){
      prev = $1;
      start = $1;
   }
   speed = (delay-($1-prev))/delay;
   o_speed = (start-($1-passed))/passed
   if (speed == 0)    speed_d = 1;
     else             speed_d = speed;
   eta = $1/speed_d;
   if (eta<0)         eta = -86400;
   o_eta = $1/o_speed;
   printf("%8d %8.6f %9.3f %7.3f | %9.3f %7.3f %7.2f\n",
      $1, $1/86400, speed, eta/86400, o_speed, o_eta/86400, o_eta/3600);
   prev=$1;
   count++;
}'
}

function sync_au_loop()
{
    local info last_id next_id
    [ -n "$1" ] && start_id=$1
    while true; do
        if [ -n "$start_id" ]; then
            last_id=$start_id
            start_id=''
        else
            info=$(sed -n '${s/^.*"last_id":\([[:digit:]]*\).*$/last_id=\1/p;}' /tmp/sync_au.rpt)
            [ $? -eq 0 ] || return
            [ -n "$info" ] || return
            eval "$info";
        fi
        [ -n "$last_id" ] || return
        next_id=`expr $last_id + 1`
        echo "starting at $next_id"
        bin/rails r /tmp/sync_au --offset "$next_id" --unarchive-limit 200 --rate 2/s --save /tmp/sync_au.rpt ||
            return
        echo resting
        sleep 5
    done
}

function s3tail_collab()
{
    if [[ $# -lt 2 ]]; then
        echo 's3tail_collab_store { s3 | elb [<region>] } <env> [<days_ago_start> [<days_ago_stop>]]' >&2
        return 1
    fi
    local svc=$1; shift
    local region='us-west-1'
    if [[ "$svc" == elb ]]; then
        if [[ "$1" =~ '-' ]]; then
            region=$1; shift
        fi
    fi
    local env
    case "$1" in
        p*) env=production;;
        s*) env=staging;;
        a*) env=andy;;
        d*) env=development;;
        *)
            echo "Unknown env: $1" >&2
            return 2
    esac
    shift
    local cmd=(s3tail --log-file ~/.s3tail_collab.log --region "$region")
    local day=${1-0}; shift
    local end=${1-0}; shift
    while [[ $day -ge $end ]]; do
        date=$(daysago -u "${day}")
        if [[ "$svc" == s3 ]]; then
            "${cmd[@]}" "unitycloud-collab-logs/${env}/s3/collab-${env}-s3-access-${date}"
        else
            local path="unitycloud-elb-logs-${region}/production-collabcache/AWSLogs/096016851792/"
            path+="elasticloadbalancing/${region}/"
            path+=$(ruby -rtime -e "print Time.parse('${date}').strftime('%Y/%m/%d/')")
            "${cmd[@]}" "$path"
        fi
        (( day-- ))
    done
}

function s3ls_region()
{
    if [[ $# -lt 2 ]]; then
        echo 's3tail_region { d[ev] | s[taging] | p[roduction] } { all | usw1 | usw1cc | euc1 | apse1 | sae1 }' >&2
        return 1
    fi
    local env region
    case "$1" in
        d*) env=dev;;
        s*) env=staging;;
        p*) env=production;;
        *)
            echo "Unknown environment: $1" >&2
            return 2;;
    esac
    case "$2" in
        all)
    esac

}

function artifactory_latest()
{
    if [[ $# -ne 1 ]]; then
        echo 'artifactory_latest <image>' >&2
        return 1
    fi
    curl -qfs "https://artifactory.eu-cph-1.unityops.net:5010/buildpipe-registry1/${1}/" | \
        awk -F\" '$2 ~ /^[0-9]*\/$/ {gsub(/\/$/,"",$2);if($2 + 0 > v) v = $2}END{print v}'
}

KCTX_ZONE_MAP=()

function gsetbsn()
{
    local jpn='.metadata.annotations.ingress\.kubernetes\.io/backends'
    if [[ $# -lt 1 || $# -gt 1 ]]; then
        # k ing  '-ojsonpath={.items[*].metadata.annotations.ingress\.kubernetes\.io/backends}'
        jpn=".items[*]${jpn}"
    fi
    GBSN=$(k ing "$@" -o"jsonpath={${jpn}}" | cut -d\" -f2)
    if [[ -z "$GBSN" ]]; then
        echo 'Unknown ingress name' >&2
        return 1
    fi
}


function gsetregion()
{
    local s='
BEGIN {
  a["usc1"] = "us-central1"
  a["euw1"] = "europe-west1"
  a["ape1"] = "asia-east1"
}
END { print a[v] }
'
    local ctx=$(k ctx)
    GREGION=$(awk "$s" v="${ctx##*-}" </dev/null)
    if [[ -z "$GREGION" ]]; then
        echo "Unknown region for \"${ctx}\"" >&2
        return 1
    fi
}

function gkloud()
{
    local ctx=$(k ctx)
    local s='
BEGIN {
  a["prd-ape1"] = "unity-gke-prd"
  a["prd-euw1"] = "unity-gke-prd"
  a["prd-usc1"] = "unity-gke-prd"
  a["stg-ape1"] = "unity-gke-stg"
  a["stg-euw1"] = "unity-gke-stg"
  a["stg-usc1"] = "unity-gke-stg"
  a["test-ape1"] = "unity-gke-test"
  a["test-euw1"] = "unity-gke-test"
  a["test-usc1"] = "unity-gke-test"
  a["tcmn-usc1"] = "unity-cs-common-test"
}
END { print a[v] }
'
    local ctx=$(k ctx) proj=$(awk "$s" v="$ctx" </dev/null)
    if [[ -z "$proj" ]]; then
        echo "Unknown project ID for \"${ctx}\"" >&2
        return 1
    fi
    echo "gcloud --project$(printf ' %q' "$proj" "$@")" >&2
    gcloud --project "$proj" "$@"
}

function gkssh()
{
    gkloud compute ssh --internal-ip "$@"
}

function gnegs()
{
    gsetregion || return
    gsetbsn "$@" || return
    (
        for zone in $(gkloud compute zones list --filter "region=${GREGION}" --format 'value(selfLink.scope())'); do
            ( gkloud beta compute network-endpoint-groups list-network-endpoints \
                     --zone "$zone" "$GBSN" 2>&1 | prefix "${zone}: " ) &
        done
        wait
    )
}

function gbs()
{
    gsetbsn "$@" || return
    gkloud beta compute backend-services describe "$GBSN" --global
}

function ghealth()
{
    gsetbsn "$@" || return
    k ing w
    k d ep | grep -E '^Name:|^ *Addresses' | cat
    gkloud beta compute backend-services get-health "$GBSN" --global | \
        match -g \ HEALTHY -b UNHEALTHY
}

function heydist()
{
    local opts=()

    while [[ $# -gt 2 && "$1" != http* ]]; do
        opts+=("$1")
        shift
    done

    if [[ $# -lt 2 ]]; then
        cat <<EOF >&2
usage: heydist [<hey_option>...] <base_uri> <path> [<path>...]
EOF
        return 1
    fi

    local base=$1; shift

    local path
    for path in "$@"; do
        hey "${opts[@]}" "${base}/${path}" &
    done

    wait
}

function generate_baton_csv()
{
    local hdrs
    while true; do
        if [[ "$1" = '-H' ]]; then
            shift; hdrs+=",$1"; shift
        else
            break
        fi
    done

    if [[ $# -lt 4 ]]; then
        cat <<EOF >&2
usage: generate_baton_csv [-H <header>] <csv> <env> <base_uri> <path> [<path> ...]
EOF
        return 1
    fi

    local csv=$1; shift
    local env=$1; shift
    local base=$1; shift

    local auth=$(ucurl $env get auth)

    local guid path cnt=0
    for guid in $(ucurl $env get \${GENESIS_API_URL}/v1/core/api/projects | \
      jq -r '.projects[] | select (.archived | not) | select (.service_flags.collab) | .guid'); do
        for path in "$@"; do
            echo "GET,${base}/api/projects/${guid}/${path},,${auth}${hdrs}"
            (( ++cnt ))
        done
    done > "$csv"

    echo "Stored ${cnt} requests in ${csv}"
}
