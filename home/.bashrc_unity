#  -*- mode: shell-script -*-

# TODO: fix utail to allow args:
# > ueach -a 'production-collab-[1-9]' -- "sudo sh -c \"tail -Fn0 /var/log/nginx/genesis-proxy.access.log\"" | pv -lra >/dev/null

[[ "$AWS_ENV" == development* ]] && awsregion us-west-1

[ -d /opt/unity/unitycloud-ops ] && export UNITYCLOUDOPS=/opt/unity/unitycloud-ops

EC2_ROLE="${EC2_ROLE#fullstack-}"

if [ -n "$EC2_ROLE" ]; then
    SECRET_FILE="$(find /var/opt -name "${EC2_ROLE//-jobs}-secret_token.${EC2_ENV}" | xargs -r ls -t | head -1)"
    [ ! -f "$SECRET_FILE" ] && \
        SECRET_FILE="$(ls -1t "/var/opt/unity-${EC2_ROLE}/"*."${EC2_ENV}" 2>/dev/null | head -1)"
    [ -f "$SECRET_FILE" ] && . "$SECRET_FILE" || unset SECRET_FILE
fi

export HEKETI_CLI_SERVER='http://gluster-api.us-west-1.unityops.net:8080'

if [[ -f "${HOME}/.ssh/id_github" ]]; then
    export GH_SSH_KEY_PATH="${HOME}/.ssh/id_github"
    export GH_SSH_KEY=$GH_SSH_KEY_PATH
fi

alias reset_dev='rm -rfv devrepos/* db/*.sqlite3; mysql -uroot -e "drop database unity_cloud_collab"; rake db:create db:migrate'
alias mcls='ruby -I /opt/unity/unity-collab-service/current/vendor/bundle/ruby/2.3.0/gems/dalli-2.7.6/lib ~/bin/memcache_list_keys'
alias list_editor_args='find Editor/Src -type f -name "*.cpp" -print0 | xargs -0 grep -F HasARGV'
alias opsme="exec bash -ic 'PATH=\"${PATH}:/opt/unity/unitycloud-ops/bin\" bash'"
alias gitsum='git ls-files -z | xargs -0 grep -F size | pipesum 10000 2'

if $IAMME && $INTERACTIVE && ihave _z; then
    zf="${_Z_DATA:-${HOME}/.z}"
    touch "$zf"
    shopt -s nullglob # empty list if no match
    for d in /opt/unity/unity*; do
        n="$(basename "$d" | cut -d- -f2-)"
        [ -d "${d}/current" ] && d="${d}/current"
        # only add once to avoid increasing the frecency value
        grep -q "$d" "$zf" || _z --add "$d" 2>/dev/null
    done
    shopt -u nullglob
fi

if $DARWIN; then
    function unity_bin()
    {
        local dn bn
        bn='Contents/MacOS/Unity'
        if [ -x "$bn" ]; then
            echo "bn"
            return 0
        fi
        for dn in '.' build/MacEditor /Applications/Unity*; do
            dn+="/Unity.app"
            if [ -d "$dn" ]; then
                echo "${dn}/${bn}"
                return 0
            fi
        done
        echo "Unable to locate Unity binary" >&2
        return 1
    }

    # if last arg is a path, will open it as a project
    function unity()
    {
        local pdir args=("$@")
        if [[ ${#args[@]} -gt 0 && -d "${args[-1]}" ]]; then
            pdir="${args[-1]}"
            unset args[-1]
            # unity requires full paths
            pdir="$(cd "$pdir" && pwd)"
            # this is how to open more than one unity project
            args+=(-projectpath "$pdir")
        fi
        local ub
        ub="$(unity_bin)"
        echo "${ub} $(shellwords "${args[@]}") >&2"
        tailrun -p "predate $ISO8601_FMT" "${HOME}/Library/Logs/Unity/Editor.log" "${ub}" "${args[@]}"
    }
fi

function health()
{
    local pn="/opt/unity/unity-${EC2_ROLE}-service/current"
    local px
    [ ! -d "$pn" ] && pn="/opt/unity/unity-${EC2_ROLE}/current"
    if [ ! -d "$pn" ]; then
        pn='/opt/unity/unity-collab-cache/current'
        px='localhost/csapi/health'
    else
        px='localhost:8080/api/health'
    fi
    myps 'unicorn|collabcache'
    ls -ld `readlink "$pn"`
    local h="$(curl -sqf -u "build:${BUILD_SERVICE_SECRET}" ${px})"
    echo "$h"
    eval `echo "$h" | sed -n 's/.*version":"\([^"]*\).*/local version=\1/p'`
    echo -n "${version}  ->"
    (cd "$pn"; git branch -r --contains "$version"; git log -n1)
}

function whitelist_check()
{
    # TODO: _any_ access token will work, so snag the first one in memcache not expired...
    if [ -z "$ACCESS_TOKEN" -o $# -lt 1 ]; then
        echo 'usage: ACCESS_TOKEN=sekritz whitelist_check <org_fk> [<org_fk>...]' >&2
        return 1
    fi
    local org_fks="$(join , "$@")"
    curl -H "Authorization: Bearer $t" "localhost:5001/v1/alpha-features/api/whitelist/COLLAB/${org_fks}"
    echo
}

function core()
{
    local cargs
    local user_fk='collab'
    if [ "$1" = '-v' ]; then
        shift; cargs='-v'
    else
        cargs='-qsf'
    fi
    if [ "$1" = '-u' ]; then
        shift; user_fk=$1; shift
    fi
    if [ $# -ne 1 ]; then
        echo 'usage core [-v] [-u <user_fk>] <path>'
        return 1
    fi
    curl $cargs -L -u "${user_fk}:${CORE_API_SECRET}" "https://${AWS_ENV_PREFIX}core.cloud.unity3d.com$1"
    local rc=$?
    echo
    return $?
}

function sshrails()
{
    local rhost srvc
    if [ $# -lt 1 ]; then
        echo 'usage: sshrails <remote_host> [<rails_options>...]' >&2
        return 1
    fi

    rhost="$1"; shift

    \ssh -t "$rhost" bash -ic "'
sudo -u nobody -i bash -c \"\
cd /opt/unity/unity-\${EC2_ROLE}-service/current;\
. \$SECRET_FILE;\
export RAILS_ENV=\$EC2_ENV;\
bin/rails "$@"\"'"
}

function volfor()
{
    local pfk="$(downcase "$1")"
    local shard="${pfk:0:2}"
    if $IAMME; then
        sudo su -c 'ls -1dt /mnt/repos/volume*'/${shard}/${pfk} | head -1
    else
        ls -1dt /mnt/repos/volume*/${shard}/${pfk} | head -1
    fi
}

function beanview()
{
    if [ $# -ne 1 ]; then
        echo 'usage: beanstalkd_view <host>' >&2
        return 1
    fi

    local privip
    if [[ $1 = production-* || $1 = staging-* ]]; then
        privip=`grep -A1 -F $1.private ~/.ssh/config | tail -1 | awk '{print $2}'`
    else
        privip=127.0.0.1
    fi

    local port=$(( ((RANDOM<<15)|RANDOM) % 63001 + 2000 ))
    \ssh -L$port:$privip:11300 -o 'ExitOnForwardFailure yes' -nfN $1 || return $?
    local sshpid=`pgrep -f ExitOnForwardFailure`

    BEANSTALK_URL="beanstalk://localhost:$port/" \beanstalkd_view -F
    kill $sshpid
}
complete -F _ssh beanview

function invalidate_cdn()
{
    if [ $# -lt 1 ]; then
        echo 'usage: invalidate_cdn <env> [<env> ...]' >&2
        return 1
    fi

    local objects=()
    while [ $# -gt 0 ]; do
        objects+=('"https://public-cdn.cloud.unity3d.com/config/'$1'"')
        shift
    done

    http -va "public-cloud-api@unity3d.com:${CDN_API_SECRET}" \
         https://api.ccu.akamai.com/ccu/v2/queues/default \
         'action:="invalidate"' 'objects:=['"$(join , "${objects[@]}")"']'
}

if ihave papertrail; then
    alias pt-prod='pt -g "Collab Production"'
fi

# TODO: add helper to cp files
# for h in `host 'production-(jobs-)?collab(-jobs)?-\d+$' | awk '{print $1 ".private"}'`; do ( scp ${h}:/opt/unity/unity-collab-service/current/log/production.log /Users/brad/work/logs/${h}-rails.log & ); done

if [ "$(id -un)" = 'nobody' ]; then
    function collab_exec()
    {
        z current && ./bin/bundle exec "$@"
    }

    function clone_project()
    {
        git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1
    }
else
    function collab_exec()
    {
        sudo -u nobody bash -ic "cd '$(z -e current)' && ./bin/bundle exec $(shellwords "$@")"
    }

    function clone_project()
    {
        sudo -u nobody git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1 && sudo chown -R "$(id -un):" .
    }
fi

function collab_rbtrace_workers
{
    collab_exec rbtrace --ps='unicorn worker' -e "$@"
}

function collab_rbtrace_master
{
    collab_exec rbtrace --ps="$(pgrep -f unicorn\ master)" -e "$@"
}


# each_worker?
# > kubey -c usw1 -n staging collab/collab each -ap -- 'for p in `pgrep -f unicorn\ worker`; do echo $p; kill -QUIT $p; done; wait'

# repl_first...
# > kubey -c usw1 -n staging -m1 coll/coll repl bash


# TODO: add helper for running this:
# ueach -a -p production-collab-\\d+ -- 'sudo -u nobody bash -ic "/opt/unity/unity-collab-service/current/bin/bundle exec rbtrace -p \$(pgrep -f unicorn\ master) -e Rails.configuration.memcached_client"'

function collab_loglevel()
{
    collab_rbtrace "Rails.logger.level = :$1"
}

# TODO:
# upid="$(basename `pwd -P`)"
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1}}' > ~/${upid}.md5s)
# one-time:
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1};fflush();}' | s3-unarchive $(basename $PWD))

# TODO: add easy ability to convert only one file!
function convert_pointers()
{
    local download=false
    if [ "$1" = '--download' ]; then
        download=true; shift
    fi

    local max_failures=10
    if [ "$1" = '--max-failures' ]; then
        shift; max_failures=$1; shift
    fi

    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers [--download] [--max-failures <count>] <directory>' >&2
        return 1
    fi

    # convert dev-user to be dev/user for bucket
    local bucket="unitycloud-collab-store-${AWS_ENV/-${USER}//${USER}}"
    local dir="$(cd "$1" && pwd -P)"; shift
    local pfk="$(basename "$dir")"
    local failures=0

    local total=`git ls-files "$dir" | wc -l`
    local count=0
    # read using NUL terminators to properly handle UTF-8 encoding of file names
    while read -r -d $'\0' fn; do
        (( count++ ))
        md5="$(awk '/^uc_md5/{print $2}' "$fn")"
        if [ -z "$md5" ]; then
            echo "No MD5 found in ${fn}" >&2
            (( failures++ ))
        elif $download; then
            mv -v "$fn" "${fn}.pointer"
            key="${bucket}/${pfk}/${md5}"
            if ! aws s3 cp "s3://$key" "$fn"; then
                mv -v "${fn}.pointer" "$fn"
                (( failures++ ))
            fi
        else
            key="${pfk}/${md5}"
            if ! aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                # echo "Missing ${bucket}/${key} in ${fn}" >&2
                key="collab_archive_${pfk}/${md5}"
                if aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                    echo "${fn} is ARCHIVED as ${bucket}/${key}" >&2
                else
                    echo "${fn} is MISSING ${bucket}/${key} (and is not archived)" >&2
                    (( failures++ ))
                fi
            fi
        fi
        if [ $failures -ge $max_failures ]; then
            echo "Giving up after ${failures} failures" >&2
            break
        fi
        [ $(( $count % 10 )) -eq 0 ] && echo "Processed ${count} / ${total}"
    done < <(git ls-files -z)

    [ $failures -eq 0 ] && return 0

    echo "Summary: ${failures} failures reported"
    return 2
}

function unconvert_pointers()
{
    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers <directory>' >&2
        return 1
    fi

    find "$1" -name .git -prune -o -type f -name '*.pointer' -print | while read fn; do
        dir="$(dirname "$fn")"
        orig="${dir}/$(basename "$fn" .pointer)"
        mv -vf "$fn" "$orig"
    done
}

# moves all files found in transaction folders out into the project root
function convert_transactions()
{
    if [ $# -lt 1 ]; then
        echo 'usage: convert_transactions <project_id> [<project_id>...]' >&2
        return 1
    fi

    echo 'TODO: make converter for old transaction folders in s3!'
}

# reports the following space-delimited fields for ONLY project/md5 requests:
#   <timestamp> <remote_ip> <http_method> <project_id> <file_md5> <http_status> <bytes_sent>
# see-also: http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html
function simplified_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: simplified_s3_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | pipejoin $'\3' | awk -F$'\3' '$9 ~ /\// {
sub(/\[/,"",$3); sub(/\]/,"",$4); gsub("/"," ",$9); split($8,a,".");
print $3 $4, $5, a[2], $9, $10, $11, $13, $(NF-1);
}'
}

function split_s3_key_from_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: split_s3_key_from_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | awk '$9 ~ /\//{gsub("/"," ",$9);a[$9]++} END{for (n in a) print n}'
}

function get_project_ids_from_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: get_project_ids_from_s3_logs <log> [<log>...]' >&2
        return 1
    fi
    split_s3_key_from_log "$@" | awk '{print $1}' | sort | uniq -c | sort -n
}

function report_project_versions()
{
    local lfn
    [ -f "$1" ] && lfn="$1" || lfn='-'

    if test $# -eq 0 && istty stdin; then
        echo 'usage: report_project_versions [<access_log_file>]' >&2
        return 1
    fi

    while read -r line; do
        res="$(echo "$line" | sed -n 's/^.*projects\/\([^\/]*\).*UnityEditor\/\([^ ]*\).*/upid="\1";ver="\2"/p')"
        eval "$res"
        if [ -z "$upid" -o -z "$ver" ]; then
            continue
        fi
        tag="$(hg_user_agent_revision -tag "$ver")"
        rc=$?
        if [ -z "$tag" -o $rc -ne 0 ]; then
            echo "Failed to get tag for ${ver}" >&2
        else
            echo "${upid}:${tag}:${ver}"
        fi
    done < <(cat "$lfn")
}

function statsd_report()
{
    local host='internal-stats.cloud.unity3d.com'
    if [ "$1" = '-h' ]; then
        shift; host="$1"; shift
    fi
    # https://github.com/b/statsd_spec | https://github.com/etsy/statsd/blob/master/docs/metric_types.md
    declare -A cmds
    cmds=([count]=c [gauge]=g [time]=ms [hist]=h [meter]=m [set]=s)
    local cmd=${cmds[$1]}
    local keys
    if [ $# -lt 2 -o -z "$cmd" ]; then
        keys="$(join '|' `echo "${!cmds[@]}"`)"
        echo "usage: stats_report [-h <host>] {${keys}} <metric> [<value>] [<sample_rate>]" >&2
        return 1
    fi
    local msg="${HOSTNAME//./_}.${2}:${3:-1}|${cmd}"
    [ -n "$4" ] && msg+="|@$4"
    echo "$msg"
    echo "$msg" | nc -uw0 "$host" 8125
}

function beanstool()
{
    local beanbin="${HOME}/bin/beanstool"
    if [ -f /etc/default/beanstalkd ]; then
        .  /etc/default/beanstalkd
    else
        BEANSTALKD_LISTEN_ADDR="$(get_iface_ip inet eth0)"
        BEANSTALKD_LISTEN_PORT='11300'
    fi
    if [ ! -x "$beanbin" ]; then
        (
            set -e
            cd "${HOME}/bin"
            ver='0.2.0'
            fn="beanstool_v${ver}_$(downcase "$UNAME")_amd64"
            wget "https://github.com/src-d/beanstool/releases/download/v${ver}/${fn}.tar.gz"
            tar zxf "${fn}.tar.gz" --strip-components=1 "${fn}/beanstool"
            rm -f "${fn}.tar.gz"
        )
    fi
    "$beanbin" "$@" --host="${BEANSTALKD_LISTEN_ADDR}:${BEANSTALKD_LISTEN_PORT}"
}

function instances_needed()
{
    if [ $# -lt 3 ]; then
        echo 'usage: instances_needed <req_per_second> <ms_latency> <num_workers>' >&2
        return 1
    fi
    calc "$1 / ((1000 / $2) * $3)"
}

function workers_needed()
{
    if [ $# -lt 3 ]; then
        echo 'usage: workers_needed <req_per_second> <ms_latency> <instances>' >&2
        return 1
    fi
    calc "($1 / (1000 / $2)) / $3"
}

function progress_report()
{
    if [ $# -ne 3 ]; then
        echo 'usage: progress_report <format> <count_interval> <total>' >&2
        return 1
    fi
    [ $(($3 % $2)) -eq 0 ] && printf "$1" $3
    return 0
}

function s3_remove_zero_byte_files()
{
    local bucket search total line_count last_upid line size key upid md5
    if [ $# -lt 1 ]; then
        echo 'usage: s3_remove_zero_byte_files <bucket> [<upid> ...]' >&2
        return 1
    fi

    bucket="$1"; shift
    if [ $# -gt 0 ]; then
        search=()
        for upid in "$@"; do search+=($upfid); done
    else
        search=("$bucket")
    fi

    total=0
    line_count=0
    for item in "${search[@]}"; do
        last_upid=''
        while read -r line; do
            line=($line)
            size="${line[2]}"
            [ "$size" -eq 0 ] || continue
            # ts="${line[0]} ${line[1]}"
            key="${line[3]}"
            upid="${key%%/*}"
            md5="${key##*/}"
            if [ "$last_upid" != "$upid" ]; then
                last_upid="$upid"
                echo "checking ${upid}"
            fi
            progress_report "removed ${total} of %d\n" 2000 $((++line_count))
            if [ -z "$md5" ] || [ "$md5" = 'd41d8cd98f00b204e9800998ecf8427e' ]; then
                # ignore directory only or legit zero byte file
                continue
            fi
            echo "${line[*]}"
            aws s3 rm "s3://${bucket}/${key}"
            (( total++ ))
        done < <( aws s3 ls --recursive "s3://$item" )
    done
    echo "Removed ${total} of ${line_count} items from ${bucket}"
}

# TODO: add helper to get currently env deployment tag and open URL to compare to master:
# https://gitlab-prod1.eu-cph-1.unityops.net/cloudservices/collab-service/compare/20170314-Production...master

# TODO: support by node name (e.g. node-7, node-8, etc)
function kssh()
{
    local user=$USER
    local kubey_args

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -u) shift; user=$1;;
            -k) shift; kubey_args=$1;;
            *)  break;;
        esac
        shift
    done

    if [[ $# -lt 1 ]]; then
        echo 'usage: kssh [-u <user>] [-k <kubey_args>] <match> [<ssh_args> ...]' >&2
        return 1
    fi

    local match=$1; shift
    local node_ip
    node_ip=($(kubey $kubey_args -f plain --no-headers "$match" list -c host_ip | \
                   cut -d' ' -f1 | sort -u))
    if [[ ${#node_ip[@]} -ne 1 ]]; then
        echo "Invalid node IP found: ${node_ip[*]}" >&2
        return 2
    fi

    ssh "${user}@${node_ip}" "$@"
}

function krepl()
{
    local args=(-m1)
    if [[ "$1" = '-c' ]]; then
        shift; args+=('-c' $1); shift
    fi
    if [[ $# -lt 1 ]]; then
        echo 'usage: krepl [-c <context>] <namespace>[/pod_match>] [<cmd> [<args...>]]' >&2
        return 1
    fi
    local e m
    if [[ "$1" == */* ]]; then
        e=${1%%/*}
        m=${1#*/}
    else
        e=$1
        m='odd/odd'
    fi
    shift
    case $e in
        prod*) e=production;;
        stag*) e=staging;;
    esac
    args+=(-n "$e" "$m" repl --)
    if [[ "$*" == 'rcs' ]]; then
        args+=(bin/rails c -s)
    elif [[ -n "$*" ]]; then
        args+=("$@")
    else
        args+=(bash)
    fi
    kubey "${args[@]}"
}

# report all internal node IPs
function knodeips()
{
    local args
    [[ -n "$1" ]] && args="--context $1"
    kubectl $args get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
}

# run a command on each node async
function keachnode()
{
    local cmd ip line
    if [[ "$1" = '-c' ]]; then
        shift; ctx=$1; shift
    fi
    cmd=$(shellwords "$@")
    for ip in $(knodeips "$ctx"); do
        pgrep -qf "ssh: .*@${ip}" || ssh "$ip" : # start background ssh control session
        ( ( ssh "$ip" $cmd 2>&1 | while read -r line; do printf '%-15s %s\n' "${ip}:" "${line}"; done ) & )
    done
    sleep 1
    wait
}

# get uptimes for all nodes sorted by descending load (highest load at top)
function kuptime()
{
    keachnode "$@" uptime | sort -rnk 11
}

# get memory usage for all nodes sorted by ascending free memory (smallest memory available at top)
function kmem()
{
    printf '%15s %s\n' '[megabytes]' '             total       used       free     shared    buffers     cached'
    keachnode "$@" sh -c 'free -m | grep ^Mem' | sort -nk 5
}

# get file system usage for all nodes sorted by percent used (smallest space available at top)
function kdf()
{
    printf '%-15s %s\n' 'Host' 'Filesystem              Size  Used Avail Use% Mounted on'
    keachnode "$@" sh -c 'df -h / /var/lib/docker | sed 1d' | sort -rnk 6
}

function kforward()
{
    # > kubectl -n services-andy -o wide get services beanstalkd
    # get service ip and use to ssh to private ip of host
    # > ssh -o 'ControlPath none' -L 11300:100.65.6.123:11300 172.26.4.166
    :
}

# display only the names of any resource (e.g. kubectl get namespaces $knames)
knames="-ojsonpath={.items..metadata.name}"

function clean_orphan_replicasets()
{
    local kargs=(-n "$1")
    [[ $# -gt 1 ]] && kargs+=(--context "$2")
    if [[ $# -lt 1 ]]; then
        echo 'usage: clean_orphan_replicasets <namespace> [<context>]' >&2
        return 1
    fi
    kubectl "${kargs[@]}" get replicasets | grep ' 0  *0  *0 ' | tee /dev/tty | awk '{print $1}' | \
        xargs kubectl "${kargs[@]}" delete replicaset
}

function collab_deploy()
{
    if [[ $# -lt 2 ]]; then
        echo 'usage: collab_deploy <env> <img> [<pods>...]' >&2
        return 1
    fi
    local env=$1; shift
    local img=$1; shift
    local pods="${*}"
    [[ -z "$pods" ]] && pods='oddjob backburner sqsprocessor collab'
    local pod fullimg
    for pod in $pods; do
        echo "[$pod]"
        fullimg="artifactory.eu-cph-1.unityops.net:5010/$pod:$img"
        ./deploy_collab.sh --stable --image "$fullimg" apply "$env" "$pod" || break
    done
}

function watch_deploy()
{
    if [[ $# -lt 1 ]]; then
        echo 'usage: watch_deploy <kubey_args>' >&2
        return 1
    fi
    local i
    while kubey  --cache-seconds 5 "$@" .; do
        echo '======================================================================'
        i=10
        while [[ $i -gt 0 ]]; do
            printf "Refresh in %d seconds...                                      \r" $i
            sleep 1
            (( i-- ))
        done
        printf "                                                                      \r"
    done
}

function ktrace()
{
    if [[ $# -ne 2 ]]; then
        echo 'usage: ktrace <kubey_args> <trace_cmd>' >&2
        return 1
    fi
    local kargs="$1"; shift
    local tcmd="$1"; shift
    local cmd='for p in `pgrep -f unicorn\ worker`; do '
    cmd+='( bin/bundle exec rbtrace -p $p -e "'"$tcmd"'" 2>/dev/null | grep --line-buffered -F "=>" & ); '
    cmd+='done; wait'
    kubey $kargs each -ap -- $cmd
}

function check_consul_available()
{
    if [[ $# -lt 1 ]]; then
        echo 'usage check_consul_available <ctx> <env>' >&2
        return 1
    fi
    local of="/tmp/consul_available_${1}_${2}.log"
    ktrace "-c $1 -n collab-$2 collab/collab" '[Process.pid,RuntimeConfig.consul_available?]' | tee "$of"
    echo "Saved output to $of"
}

# # https://www.percona.com/blog/2012/08/29/heres-a-quick-way-to-foresee-if-replication-slave-is-ever-going-to-catch-up-and-when/
# s_behind – current Seconds_Behind_Master value
# d_behind – number of days behind based on current s_behind
# c_sec_s – how many seconds per second were caught up during last interval
# eta_d – this is ETA based on last interval
# O_c_sec_s – overall catch-up speed in seconds per second
# O_eta_d – ETA based on overall catch-up speed (in days)
# O_eta_h – same like previous but in hours
function mysql-slave-status-monitor()
{
    if [[ $# -ne 1 ]]; then
        echo 'usage: mysql-slave-status-monitor <interval-seconds>' >&2
        return 1
    fi
    local delay=$1
    local cmd="$SUDO mysql -e 'show slave status\G' | grep Seconds_Behind_Master | awk '{print \$2}'"
    while sleep $delay; do
        eval $cmd
    done | awk -v delay=$delay '
{
   passed += delay;
   if (count%10==0)
      printf("s_behind d_behind   c_sec_s   eta_d | O_c_sec_s O_eta_d O_eta_h\n");
   if (prev==NULL){
      prev = $1;
      start = $1;
   }
   speed = (delay-($1-prev))/delay;
   o_speed = (start-($1-passed))/passed
   if (speed == 0)    speed_d = 1;
     else             speed_d = speed;
   eta = $1/speed_d;
   if (eta<0)         eta = -86400;
   o_eta = $1/o_speed;
   printf("%8d %8.6f %9.3f %7.3f | %9.3f %7.3f %7.2f\n",
      $1, $1/86400, speed, eta/86400, o_speed, o_eta/86400, o_eta/3600);
   prev=$1;
   count++;
}'
}

function sync_au_loop()
{
    local info last_id next_id
    [ -n "$1" ] && start_id=$1
    while true; do
        if [ -n "$start_id" ]; then
            last_id=$start_id
            start_id=''
        else
            info=$(sed -n '${s/^.*"last_id":\([[:digit:]]*\).*$/last_id=\1/p;}' /tmp/sync_au.rpt)
            [ $? -eq 0 ] || return
            [ -n "$info" ] || return
            eval "$info";
        fi
        [ -n "$last_id" ] || return
        next_id=`expr $last_id + 1`
        echo "starting at $next_id"
        bin/rails r /tmp/sync_au --offset "$next_id" --unarchive-limit 200 --rate 2/s --save /tmp/sync_au.rpt ||
            return
        echo resting
        sleep 5
    done
}

function s3tail_collab()
{
    if [[ $# -lt 2 ]]; then
        echo 's3tail_collab_store { s3 | elb [<region>] } <env> [<days_ago_start> [<days_ago_stop>]]' >&2
        return 1
    fi
    local svc=$1; shift
    if [[ "$svc" == elb ]]; then
        local region
        if [[ "$1" =~ '-' ]]; then
            region=$1; shift
        else
            region='us-west-1'
        fi
    fi
    local env
    case "$1" in
        p*) env=production;;
        s*) env=staging;;
        a*) env=andy;;
        d*) env=development;;
        *)
            echo "Unknown env: $1" >&2
            return 2
    esac
    shift
    local cmd=(s3tail --log-file ~/.s3tail_collab.log --region "$region")
    local day=${1-0}; shift
    local end=${1-0}; shift
    while [[ $day -ge $end ]]; do
        date=$(daysago -u "${day}")
        if [[ "$svc" == s3 ]]; then
            "${cmd[@]}" "unitycloud-collab-logs/${env}/s3/collab-${env}-s3-access-${date}"
        else
            local path="unitycloud-elb-logs-${region}/production-collabcache/AWSLogs/096016851792/"
            path+="elasticloadbalancing/${region}/"
            path+=$(ruby -rtime -e "print Time.parse('${date}').strftime('%Y/%m/%d/')")
            "${cmd[@]}" "$path"
        fi
        (( day-- ))
    done
}
