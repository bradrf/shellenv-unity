#  -*- mode: shell-script -*-

# TODO: fix utail to allow args:
# > ueach -a 'production-collab-[1-9]' -- "sudo sh -c \"tail -Fn0 /var/log/nginx/genesis-proxy.access.log\"" | pv -lra >/dev/null

[[ "$AWS_ENV" == development* ]] && awsregion us-west-1

[ -d /opt/unity/unitycloud-ops ] && export UNITYCLOUDOPS=/opt/unity/unitycloud-ops

EC2_ROLE="${EC2_ROLE#fullstack-}"

if [ -n "$EC2_ROLE" ]; then
    SECRET_FILE="$(find /var/opt -name "${EC2_ROLE//-jobs}-secret_token.${EC2_ENV}" | xargs -r ls -t | head -1)"
    [ ! -f "$SECRET_FILE" ] && \
        SECRET_FILE="$(ls -1t "/var/opt/unity-${EC2_ROLE}/"*."${EC2_ENV}" 2>/dev/null | head -1)"
    [ -f "$SECRET_FILE" ] && . "$SECRET_FILE" || unset SECRET_FILE
fi

export HEKETI_CLI_SERVER='http://gluster-api.us-west-1.unityops.net:8080'

alias reset_dev='rm -rfv devrepos/* db/*.sqlite3; mysql -uroot -e "drop database unity_cloud_collab"; rake db:create db:migrate'
alias mcls='ruby -I /opt/unity/unity-collab-service/current/vendor/bundle/ruby/2.3.0/gems/dalli-2.7.6/lib ~/bin/memcache_list_keys'
alias list_editor_args='find Editor/Src -type f -name "*.cpp" -print0 | xargs -0 grep -F HasARGV'
alias opsme="exec bash -ic 'PATH=\"${PATH}:/opt/unity/unitycloud-ops/bin\" bash'"
alias gitsum='git ls-files -z | xargs -0 grep -F size | pipesum 10000 2'

if $IAMME && $INTERACTIVE && ihave _z; then
    zf="${_Z_DATA:-${HOME}/.z}"
    touch "$zf"
    shopt -s nullglob # empty list if no match
    for d in /opt/unity/unity*; do
        n="$(basename "$d" | cut -d- -f2-)"
        [ -d "${d}/current" ] && d="${d}/current"
        # only add once to avoid increasing the frecency value
        grep -q "$d" "$zf" || _z --add "$d" 2>/dev/null
    done
    shopt -u nullglob
fi

if $DARWIN; then
    function unity_bin()
    {
        local dn bn
        bn='Contents/MacOS/Unity'
        if [ -x "$bn" ]; then
            echo "bn"
            return 0
        fi
        for dn in '.' build/MacEditor /Applications/Unity*; do
            dn+="/Unity.app"
            if [ -d "$dn" ]; then
                echo "${dn}/${bn}"
                return 0
            fi
        done
        echo "Unable to locate Unity binary" >&2
        return 1
    }

    # if last arg is a path, will open it as a project
    function unity()
    {
        local pdir args=("$@")
        if [[ ${#args[@]} -gt 0 && -d "${args[-1]}" ]]; then
            pdir="${args[-1]}"
            unset args[-1]
            # unity requires full paths
            pdir="$(cd "$pdir" && pwd)"
            # this is how to open more than one unity project
            args+=(-projectpath "$pdir")
        fi
        local ub
        ub="$(unity_bin)"
        echo "${ub} $(shellwords "${args[@]}") >&2"
        tailrun -p "predate $ISO8601_FMT" "${HOME}/Library/Logs/Unity/Editor.log" "${ub}" "${args[@]}"
    }
fi

function health()
{
    local pn="/opt/unity/unity-${EC2_ROLE}-service/current"
    local px
    [ ! -d "$pn" ] && pn="/opt/unity/unity-${EC2_ROLE}/current"
    if [ ! -d "$pn" ]; then
        pn='/opt/unity/unity-collab-cache/current'
        px='localhost/csapi/health'
    else
        px='localhost:8080/api/health'
    fi
    myps 'unicorn|collabcache'
    ls -ld `readlink "$pn"`
    local h="$(curl -sqf -u "build:${BUILD_SERVICE_SECRET}" ${px})"
    echo "$h"
    eval `echo "$h" | sed -n 's/.*version":"\([^"]*\).*/local version=\1/p'`
    echo -n "${version}  ->"
    (cd "$pn"; git branch -r --contains "$version"; git log -n1)
}

function whitelist_check()
{
    # TODO: _any_ access token will work, so snag the first one in memcache not expired...
    if [ -z "$ACCESS_TOKEN" -o $# -lt 1 ]; then
        echo 'usage: ACCESS_TOKEN=sekritz whitelist_check <org_fk> [<org_fk>...]' >&2
        return 1
    fi
    local org_fks="$(join , "$@")"
    curl -H "Authorization: Bearer $t" "localhost:5001/v1/alpha-features/api/whitelist/COLLAB/${org_fks}"
    echo
}

function core()
{
    local cargs
    local user_fk='collab'
    if [ "$1" = '-v' ]; then
        shift; cargs='-v'
    else
        cargs='-qsf'
    fi
    if [ "$1" = '-u' ]; then
        shift; user_fk=$1; shift
    fi
    if [ $# -ne 1 ]; then
        echo 'usage core [-v] [-u <user_fk>] <path>'
        return 1
    fi
    curl $cargs -L -u "${user_fk}:${CORE_API_SECRET}" "https://${AWS_ENV_PREFIX}core.cloud.unity3d.com$1"
    local rc=$?
    echo
    return $?
}

function sshrails()
{
    local rhost srvc
    if [ $# -lt 1 ]; then
        echo 'usage: sshrails <remote_host> [<rails_options>...]' >&2
        return 1
    fi

    rhost="$1"; shift

    \ssh -t "$rhost" bash -ic "'
sudo -u nobody -i bash -c \"\
cd /opt/unity/unity-\${EC2_ROLE}-service/current;\
. \$SECRET_FILE;\
export RAILS_ENV=\$EC2_ENV;\
bin/rails "$@"\"'"
}

function volfor()
{
    local pfk="$(downcase "$1")"
    local shard="${pfk:0:2}"
    if $IAMME; then
        sudo su -c 'ls -1dt /mnt/repos/volume*'/${shard}/${pfk} | head -1
    else
        ls -1dt /mnt/repos/volume*/${shard}/${pfk} | head -1
    fi
}

function beanview()
{
    if [ $# -ne 1 ]; then
        echo 'usage: beanstalkd_view <host>' >&2
        return 1
    fi

    local privip
    if [[ $1 = production-* || $1 = staging-* ]]; then
        privip=`grep -A1 -F $1.private ~/.ssh/config | tail -1 | awk '{print $2}'`
    else
        privip=127.0.0.1
    fi

    local port=$(( ((RANDOM<<15)|RANDOM) % 63001 + 2000 ))
    \ssh -L$port:$privip:11300 -o 'ExitOnForwardFailure yes' -nfN $1 || return $?
    local sshpid=`pgrep -f ExitOnForwardFailure`

    BEANSTALK_URL="beanstalk://localhost:$port/" \beanstalkd_view -F
    kill $sshpid
}
complete -F _ssh beanview

function invalidate_cdn()
{
    if [ $# -lt 1 ]; then
        echo 'usage: invalidate_cdn <env> [<env> ...]' >&2
        return 1
    fi

    local objects=()
    while [ $# -gt 0 ]; do
        objects+=('"https://public-cdn.cloud.unity3d.com/config/'$1'"')
        shift
    done

    http -va "public-cloud-api@unity3d.com:${CDN_API_SECRET}" \
         https://api.ccu.akamai.com/ccu/v2/queues/default \
         'action:="invalidate"' 'objects:=['"$(join , "${objects[@]}")"']'
}

if ihave papertrail; then
    alias pt-prod='pt -g "Collab Production"'
fi

# TODO: add helper to cp files
# for h in `host 'production-(jobs-)?collab(-jobs)?-\d+$' | awk '{print $1 ".private"}'`; do ( scp ${h}:/opt/unity/unity-collab-service/current/log/production.log /Users/brad/work/logs/${h}-rails.log & ); done

if [ "$(id -un)" = 'nobody' ]; then
    function collab_exec()
    {
        z current && ./bin/bundle exec "$@"
    }

    function clone_project()
    {
        git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1
    }
else
    function collab_exec()
    {
        sudo -u nobody bash -ic "cd '$(z -e current)' && ./bin/bundle exec $(shellwords "$@")"
    }

    function clone_project()
    {
        sudo -u nobody git clone "$(volfor $1)" /tmp/$1 && cd /tmp/$1 && sudo chown -R "$(id -un):" .
    }
fi

function collab_rbtrace_workers
{
    collab_exec rbtrace --ps='unicorn worker' -e "$@"
}

function collab_rbtrace_master
{
    collab_exec rbtrace --ps="$(pgrep -f unicorn\ master)" -e "$@"
}

# TODO: add helper for running this:
# ueach -a -p production-collab-\\d+ -- 'sudo -u nobody bash -ic "/opt/unity/unity-collab-service/current/bin/bundle exec rbtrace -p \$(pgrep -f unicorn\ master) -e Rails.configuration.memcached_client"'

function collab_loglevel()
{
    collab_rbtrace "Rails.logger.level = :$1"
}

# TODO:
# upid="$(basename `pwd -P`)"
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1}}' > ~/${upid}.md5s)
# one-time:
# convert_pointers . 2>&1 | tee >(awk '/is ARCHIVED as/{split($NF,a,"/");b=a[3];if(!(b in k)){print b;k[b]=1};fflush();}' | s3-unarchive $(basename $PWD))

# TODO: add easy ability to convert only one file!
function convert_pointers()
{
    local download=false
    if [ "$1" = '--download' ]; then
        download=true; shift
    fi

    local max_failures=10
    if [ "$1" = '--max-failures' ]; then
        shift; max_failures=$1; shift
    fi

    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers [--download] [--max-failures <count>] <directory>' >&2
        return 1
    fi

    # convert dev-user to be dev/user for bucket
    local bucket="unitycloud-collab-store-${AWS_ENV/-${USER}//${USER}}"
    local dir="$(cd "$1" && pwd -P)"; shift
    local pfk="$(basename "$dir")"
    local failures=0

    local total=`git ls-files "$dir" | wc -l`
    local count=0
    # read using NUL terminators to properly handle UTF-8 encoding of file names
    while read -r -d $'\0' fn; do
        (( count++ ))
        md5="$(awk '/^uc_md5/{print $2}' "$fn")"
        if [ -z "$md5" ]; then
            echo "No MD5 found in ${fn}" >&2
            (( failures++ ))
        elif $download; then
            mv -v "$fn" "${fn}.pointer"
            key="${bucket}/${pfk}/${md5}"
            if ! aws s3 cp "s3://$key" "$fn"; then
                mv -v "${fn}.pointer" "$fn"
                (( failures++ ))
            fi
        else
            key="${pfk}/${md5}"
            if ! aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                # echo "Missing ${bucket}/${key} in ${fn}" >&2
                key="collab_archive_${pfk}/${md5}"
                if aws s3api head-object --bucket "$bucket" --key "$key" >/dev/null 2>&1; then
                    echo "${fn} is ARCHIVED as ${bucket}/${key}" >&2
                else
                    echo "${fn} is MISSING ${bucket}/${key} (and is not archived)" >&2
                    (( failures++ ))
                fi
            fi
        fi
        if [ $failures -ge $max_failures ]; then
            echo "Giving up after ${failures} failures" >&2
            break
        fi
        [ $(( $count % 10 )) -eq 0 ] && echo "Processed ${count} / ${total}"
    done < <(git ls-files -z)

    [ $failures -eq 0 ] && return 0

    echo "Summary: ${failures} failures reported"
    return 2
}

function unconvert_pointers()
{
    if [ $# -ne 1 ]; then
        echo 'usage: convert_pointers <directory>' >&2
        return 1
    fi

    find "$1" -name .git -prune -o -type f -name '*.pointer' -print | while read fn; do
        dir="$(dirname "$fn")"
        orig="${dir}/$(basename "$fn" .pointer)"
        mv -vf "$fn" "$orig"
    done
}

# moves all files found in transaction folders out into the project root
function convert_transactions()
{
    if [ $# -lt 1 ]; then
        echo 'usage: convert_transactions <project_id> [<project_id>...]' >&2
        return 1
    fi

    echo 'TODO: make converter for old transaction folders in s3!'
}

# reports the following space-delimited fields for ONLY project/md5 requests:
#   <timestamp> <remote_ip> <http_method> <project_id> <file_md5> <http_status> <bytes_sent>
# see-also: http://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html
function simplified_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: simplified_s3_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | pipejoin $'\3' | awk -F$'\3' '$9 ~ /\// {
sub(/\[/,"",$3); sub(/\]/,"",$4); gsub("/"," ",$9); split($8,a,".");
print $3 $4, $5, a[2], $9, $11, $13, $(NF-1);
}'
}

function split_s3_key_from_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: split_s3_key_from_log <log> [<log>...]' >&2
        return 1
    fi
    cat "$@" | awk '$9 ~ /\//{gsub("/"," ",$9);a[$9]++} END{for (n in a) print n}'
}

function get_project_ids_from_s3_log()
{
    if [ $# -lt 1 ]; then
        echo 'usage: get_project_ids_from_s3_logs <log> [<log>...]' >&2
        return 1
    fi
    split_s3_key_from_log "$@" | awk '{print $1}' | sort | uniq -c | sort -n
}

function report_project_versions()
{
    local lfn
    [ -f "$1" ] && lfn="$1" || lfn='-'

    if test $# -eq 0 && istty stdin; then
        echo 'usage: report_project_versions [<access_log_file>]' >&2
        return 1
    fi

    while read -r line; do
        res="$(echo "$line" | sed -n 's/^.*projects\/\([^\/]*\).*UnityEditor\/\([^ ]*\).*/upid="\1";ver="\2"/p')"
        eval "$res"
        if [ -z "$upid" -o -z "$ver" ]; then
            continue
        fi
        tag="$(hg_user_agent_revision -tag "$ver")"
        rc=$?
        if [ -z "$tag" -o $rc -ne 0 ]; then
            echo "Failed to get tag for ${ver}" >&2
        else
            echo "${upid}:${tag}:${ver}"
        fi
    done < <(cat "$lfn")
}

function statsd_report()
{
    local host='internal-stats.cloud.unity3d.com'
    if [ "$1" = '-h' ]; then
        shift; host="$1"; shift
    fi
    # https://github.com/b/statsd_spec | https://github.com/etsy/statsd/blob/master/docs/metric_types.md
    declare -A cmds
    cmds=([count]=c [gauge]=g [time]=ms [hist]=h [meter]=m [set]=s)
    local cmd=${cmds[$1]}
    local keys
    if [ $# -lt 2 -o -z "$cmd" ]; then
        keys="$(join '|' `echo "${!cmds[@]}"`)"
        echo "usage: stats_report [-h <host>] {${keys}} <metric> [<value>] [<sample_rate>]" >&2
        return 1
    fi
    local msg="${HOSTNAME//./_}.${2}:${3:-1}|${cmd}"
    [ -n "$4" ] && msg+="|@$4"
    echo "$msg"
    echo "$msg" | nc -uw0 "$host" 8125
}

function beanstool()
{
    local beanbin="${HOME}/bin/beanstool"
    if [ -f /etc/default/beanstalkd ]; then
        .  /etc/default/beanstalkd
    else
        BEANSTALKD_LISTEN_ADDR="$(get_iface_ip inet eth0)"
        BEANSTALKD_LISTEN_PORT='11300'
    fi
    if [ ! -x "$beanbin" ]; then
        (
            set -e
            cd "${HOME}/bin"
            ver='0.2.0'
            fn="beanstool_v${ver}_$(downcase "$UNAME")_amd64"
            wget "https://github.com/src-d/beanstool/releases/download/v${ver}/${fn}.tar.gz"
            tar zxf "${fn}.tar.gz" --strip-components=1 "${fn}/beanstool"
            rm -f "${fn}.tar.gz"
        )
    fi
    "$beanbin" "$@" --host="${BEANSTALKD_LISTEN_ADDR}:${BEANSTALKD_LISTEN_PORT}"
}

function capacity()
{
    if [ $# -lt 3 ]; then
        echo 'usage: capacity <req_per_second> <ms_latency> <num_workers>' >&2
        return 1
    fi
    calc "$1 / ((1000 / $2) * $3)"
}

function workers_needed()
{
    if [ $# -lt 3 ]; then
        echo 'usage: workers_needed <req_per_second> <ms_latency> <capacity>' >&2
        return 1
    fi
    calc "($1 / (1000 / $2)) / $3"
}

function progress_report()
{
    if [ $# -ne 3 ]; then
        echo 'usage: progress_report <format> <count_interval> <total>' >&2
        return 1
    fi
    [ $(($3 % $2)) -eq 0 ] && printf "$1" $3
    return 0
}

function s3_remove_zero_byte_files()
{
    local bucket search total line_count last_upid line size key upid md5
    if [ $# -lt 1 ]; then
        echo 'usage: s3_remove_zero_byte_files <bucket> [<upid> ...]' >&2
        return 1
    fi

    bucket="$1"; shift
    if [ $# -gt 0 ]; then
        search=()
        for upid in "$@"; do search+=($upfid); done
    else
        search=("$bucket")
    fi

    total=0
    line_count=0
    for item in "${search[@]}"; do
        last_upid=''
        while read -r line; do
            line=($line)
            size="${line[2]}"
            [ "$size" -eq 0 ] || continue
            # ts="${line[0]} ${line[1]}"
            key="${line[3]}"
            upid="${key%%/*}"
            md5="${key##*/}"
            if [ "$last_upid" != "$upid" ]; then
                last_upid="$upid"
                echo "checking ${upid}"
            fi
            progress_report "removed ${total} of %d\n" 2000 $((++line_count))
            if [ -z "$md5" ] || [ "$md5" = 'd41d8cd98f00b204e9800998ecf8427e' ]; then
                # ignore directory only or legit zero byte file
                continue
            fi
            echo "${line[*]}"
            aws s3 rm "s3://${bucket}/${key}"
            (( total++ ))
        done < <( aws s3 ls --recursive "s3://$item" )
    done
    echo "Removed ${total} of ${line_count} items from ${bucket}"
}

# TODO: add helper to get currently env deployment tag and open URL to compare to master:
# https://gitlab-prod1.eu-cph-1.unityops.net/cloudservices/collab-service/compare/20170314-Production...master

# TODO: support by node name (e.g. node-7, node-8, etc)
function kssh()
{
    local user='admin'

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -n) shift; namespace="$1";;
            -u) shift; user="$1";;
            *)  break;;
        esac
        shift
    done

    if [[ $# -lt 1 ]]; then
        echo 'usage: kssh [-u <user>] <match> [<ssh_args> ...]' >&2
        return 1
    fi

    local match="$1"; shift
    local node_ip
    node_ip=($(kubey -f plain --no-headers "$match" list -c node-ip | sort -u))
    if [[ ${#node_ip[@]} -ne 1 ]]; then
        echo "Invalid node IP found: ${node_ip[*]}" >&2
        return 2
    fi

    ssh "${user}@${node_ip}" "$@"
}

function find_large_docker_logs()
{
    local size=${1:-5G}
    local fn
    echo
    while read -r fn; do
        find -L /var/log/containers -xtype l -samefile "$fn" -print0 | xargs -0 ls -l
        ls -lh "$fn"
        echo
    done < <(find /var/lib/docker/containers -type f -size "+$size")
}
